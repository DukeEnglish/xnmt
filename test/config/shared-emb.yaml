# sharing exact same embedder between to submodels
defaults:
  experiment:
    model_file: examples/output/<EXP>.mod
    hyp_file: examples/output/<EXP>.hyp
    out_file: examples/output/<EXP>.out
    err_file: examples/output/<EXP>.err
    run_for_epochs: 2
  train:
    default_layer_dim: 32
    training_corpus: !BilingualTrainingCorpus
      train_src: examples/data/head.ja
      train_trg: examples/data/head.en
      dev_src: &dev_src examples/data/head.ja  # here we share the value of a field
      dev_trg: &dev_trg examples/data/head.en 
    corpus_parser: !BilingualCorpusParser
      src_reader: !PlainTextReader {}
      trg_reader: !PlainTextReader {}
    model: !DefaultTranslator
      src_embedder: !SimpleWordEmbedder &src_emb1 
        # src and trg embedders will point to the exact same object and memory and use the same DyNet parameters
        __xnmt_id: src_emb # this is realized by specifying a __xnmt_id 
        emb_dim: 32
      encoder: !ModularEncoder
        modules:
        # In contrast, here we only share the configuration of two components, but both are still initialized separately
        # and do not share any parameters. This is the result of not specifying the __xnmt_id
        - !LSTMEncoder &lstm_enc1
            layers: 1
        - *lstm_enc1
      attender: !StandardAttender {}
      trg_embedder: *src_emb1
      decoder: !MlpSoftmaxDecoder
        layers: 1
        bridge: !CopyBridge {}
  decode:
    src_file: *dev_src
  evaluate:
    ref_file: *dev_trg

a.pretrain:

b.load:
  train:
    pretrained_model_file: examples/output/a.pretrain.mod
